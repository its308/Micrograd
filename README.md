MicroGrad: A Minimal Autograd Engine

This repository contains an implementation and exploration of MicroGrad, a minimalistic autograd engine for building and training neural networks. The project focuses on understanding automatic differentiation and implementing a basic computational graph.

Features

Implementation of basic autograd functionality.

Numerical computation and differentiation.

Visualization and analysis of function gradients.

Installation

To run this project, install the required dependencies:

pip install numpy pandas matplotlib

Usage

Run the Jupyter Notebook to execute the project:

jupyter notebook micrograd.ipynb

Example

A simple function used for gradient computation:

def f(x):
    return 3*x*x - 4*x + 5

Contributing

Contributions are welcome! Feel free to fork this repository and improve the implementation.

License

This project is licensed under the MIT License.

